# GPT-2モデル

## 学習目標

GPT-2アーキテクチャ全体を実装し、**Decoder-only Transformer**の構造を理解する。

## 主要概念

### 1. GPTクラスの全体像

```python
class GPT(nn.Module):
    def __init__(self, vocab_size, context_len, embed_dim, n_head, n_layer, ff_dim, dropout_rate):
        super().__init__()
        # 埋め込み層
        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.pos_embed = nn.Embedding(context_len, embed_dim)
        self.dropout = nn.Dropout(dropout_rate)

        # Transformerブロック
        self.blocks = nn.ModuleList([
            Block(embed_dim, n_head, ff_dim, dropout_rate)
            for _ in range(n_layer)
        ])

        # 出力層
        self.norm = nn.LayerNorm(embed_dim)
        self.unembed = nn.Linear(embed_dim, vocab_size)

        # 重み共有
        self.embed.weight = self.unembed.weight
```

### 2. 順伝播

```python
def forward(self, ids, targets=None):
    B, C = ids.size()
    device = ids.device

    # 埋め込み
    pos = torch.arange(0, C, dtype=torch.long, device=device)
    emb = self.embed(ids)           # トークン埋め込み
    pos_emb = self.pos_embed(pos)   # 位置埋め込み
    x = self.dropout(emb + pos_emb)

    # Transformerブロック
    for block in self.blocks:
        x = block(x)
    x = self.norm(x)

    # 出力（語彙サイズへの射影）
    logits = self.unembed(x)  # (B, C, vocab_size)
    return logits
```

### 3. 重みの初期化

```python
def _init_weights(self, module):
    if isinstance(module, nn.Linear):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

### 4. 保存と読み込み

```python
def save(self, file_path):
    checkpoint = {
        'model_state_dict': self.state_dict(),
        'vocab_size': self.vocab_size,
        # ... 他のハイパーパラメータ
    }
    torch.save(checkpoint, file_path)

@classmethod
def load_from(cls, file_path, device='cpu'):
    checkpoint = torch.load(file_path, map_location=device)
    model = cls(**checkpoint)  # ハイパーパラメータから再構築
    model.load_state_dict(checkpoint['model_state_dict'])
    return model
```

### 5. モデル構成例

```python
vocab_size = 1000
context_len = 256
embed_dim = 384
n_head = 6
n_layer = 6
ff_dim = 4 * embed_dim  # = 1536
dropout_rate = 0.1

model = GPT(vocab_size, context_len, embed_dim, n_head,
            n_layer, ff_dim, dropout_rate)
```

## アーキテクチャ図

```
入力ID: (B, C)
    ↓ Token Embedding + Position Embedding
埋め込み: (B, C, E)
    ↓ Dropout
    ↓ Block × n_layer
        ├── LayerNorm → MultiHeadAttention → 残差接続
        └── LayerNorm → FFN → 残差接続
    ↓ LayerNorm
    ↓ Unembed (Linear)
出力: (B, C, vocab_size)
```

## ポイント

1. **重み共有**: embed と unembed で同じ重みを使用（パラメータ削減）
2. **位置埋め込み**: 学習可能な埋め込み（Sinusoidal ではない）
3. **n_layer**: ブロック数が深さを決定（GPT-2 Small: 12層）
